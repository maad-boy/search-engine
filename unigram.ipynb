{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk import PorterStemmer  \n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from nltk.probability import LidstoneProbDist, WittenBellProbDist\n",
    "import sys\n",
    "from IPython.display import display\n",
    "#sys.stdout = stdout\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "stopWord = set(sw.words(\"english\"));\n",
    "import numpy as np\n",
    "import pandas as pds\n",
    "import math\n",
    "import os\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [\"ray_913742943690653739_cf.txt\", \"ray_925987435331224445_cf.txt\", \n",
    "        \"ray_925987537478928034_pf.txt\"]\n",
    "lam = 0.3\n",
    "topNRDoc = 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Functions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to clean document \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting word from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dots = ['.', ',', '!', '', \"+\", \"#\", \"(\", \")\", \":\", \"'s\"]\n",
    "slangs = {\"n't\":\"not\", \"r\": \"are\", \"u\": \"you\"}\n",
    "def goclean(doc):\n",
    "    doc = doc.lower()\n",
    "    #\\W+\n",
    "    tokens = wt(doc)\n",
    "    filterWord = [];\n",
    "    \n",
    "    for w in tokens:\n",
    "        if w not in dots and w not in stopWord:\n",
    "            if w in slangs:\n",
    "                w = slangs[w];\n",
    "            filterWord.append(w)\n",
    "    sents = \" \".join(filterWord)\n",
    "    filterWord = re.findall('\\w+', sents)\n",
    "    ps = PorterStemmer()\n",
    "    stemed = []\n",
    "    for w in filterWord:\n",
    "        fword = ps.stem(w)\n",
    "        stemed.append(fword) \n",
    "    return stemed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to calculate probability\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getprobability(cleanW):\n",
    "    myarray = np.asarray(cleanW)\n",
    "    ue, ce = np.unique(myarray, return_counts=True)\n",
    "    tc = len(cleanW);\n",
    "    n = float(tc)\n",
    "    \n",
    "    p = [];\n",
    "    for i in range(len(ce)): \n",
    "        p.append(ce[i]/n)\n",
    "    \n",
    "    mydict = dict(zip(ue, p))\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smoothing function not in use now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(probd, probc, lam):\n",
    "    dict2 = dict(probc)\n",
    "    \n",
    "    for key, items in probc.iteritems():\n",
    "        if key in probd:\n",
    "            val = (1 - lam)*probd[key] + lam*probc[key];\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "        \n",
    "        else:\n",
    "            val = lam*(probc[key]);\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "    return dict2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change in formula instead of smoothing we get prob ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getpratio(probd, probc, lam):\n",
    "    dict2 = dict(probc)\n",
    "    \n",
    "    for key, items in probc.iteritems():\n",
    "        if key in probd:\n",
    "            val = ((1 - lam)*probd[key])/(lam*probc[key]) + 1;\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "        \n",
    "        else:\n",
    "            val = 1\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "    return dict2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inverse indexing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping words to document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordMap(clrev, wordMap, i):\n",
    "    for key in clrev:\n",
    "        wordMap[key].append(i)\n",
    "    return wordMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merging wordMap for a query to get the list of doc which contain the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMerge(wordMap, word):\n",
    "    docList = []\n",
    "    for w in word:\n",
    "        if w not in pdf.index:\n",
    "            continue;\n",
    "        docList= list(set(docList + wordMap[w]))\n",
    "    return docList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence / Ranking function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KLdivergence(q, df, docList):\n",
    "    TD = len(docList)\n",
    "    x = [0]*TD;\n",
    "    for word, prob in q.iteritems():\n",
    "        if word not in pdf.index:\n",
    "            continue;\n",
    "        for i in range(TD):\n",
    "            name = 'probd' + str(docList[i])\n",
    "            x[i] = x[i] + prob*(math.log(pdf[name][word]))\n",
    "    return x;       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working on collection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"\"\n",
    "def getdoc(files, doc):\n",
    "    for f in files:\n",
    "        doc += open(f , 'r').read()\n",
    "    return doc\n",
    "\n",
    "doc += getdoc(files, doc);\n",
    "doc = doc.decode('utf-8');\n",
    "cw = goclean(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probc = getprobability(cw)\n",
    "dict2 = dict(probc)\n",
    "pdf = pds.DataFrame(dict2.values(), index=dict2.keys(), columns=[\"probc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc = len(probc)\n",
    "values = [[] for _ in range(tc)]\n",
    "keys = dict2.keys()\n",
    "wordMap = dict(zip(keys, values)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on documents / Doc model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = doc.split(\"\\n\")\n",
    "td = len(reviews)\n",
    "for i in range(len(reviews)):\n",
    "    name = \"probd\" + str(i);\n",
    "    \n",
    "    clrev = goclean(reviews[i]) # ['what', 'should', 'not', 'be', 'done'] without removing stopword\n",
    "    \n",
    "    wordMap = getWordMap(clrev, wordMap, i) #{u'andra': [1084, 1084], u'authorit': [254, 255, 784, 254, 255, 784],..}\n",
    "    \n",
    "    prob = getprobability(clrev) #{'be': 0.20000000000000001,'done': 0.20000000000000001,........}\n",
    "    \n",
    "    ps = getpratio(prob, probc, lam) #((1 - lam)*probd[key])/(lam*probc[key]) + 1;\n",
    "    \n",
    "    pdf[name] = ps.values() #including the doc model in data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good work\n"
     ]
    }
   ],
   "source": [
    "#q = (reviews[16] + '.')[:-1]\n",
    "q = raw_input() # \"what shouldn't be done\"\n",
    "\n",
    "cq = goclean(q) # ['what', 'should', 'not', 'be', 'done'] without removing stopword\n",
    "\n",
    "qprob = getprobability(cq) #{'be': 0.20000000000000001,'done': 0.20000000000000001,........}  \n",
    "\n",
    "docList = getMerge(wordMap, cq) #[1195, 4, 519, 522, 524, .......] list containg words in query\n",
    "\n",
    "div = KLdivergence(qprob, pdf, docList) #ranking function using KL divergens\n",
    "\n",
    "index = div.index(max(div)) #most probabel relevent doc\n",
    "\n",
    "#reviews[docList[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top N Relevent doc\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1) good work JC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'2) good strong, engaging pitch. Good work here'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'3) Nice work. Good use of customer examples.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'4) Good composure and good flow. Good speech avoiding \\u201c\\u2026euh\\u2026\\u201d during the messaging. Good work on the eye contact. Case studies are well presented.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'5) Hit the elements of the solutions and strategy. Good work including customer references.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'6) Good work on the second attempt. You kept it concise and you hit on the primary differentiators.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'7) Hi Piyabut, good work in going through this and coming out as a clear and detailed presenter!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'8) Like the affiliation with a specific customer, good work going off-script.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'9) Good work Chalita! I like the way you asked the question to start the pitch.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'10) Good work Vincent! Like the way you pace it and how you go down the different layers.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tlist = div[:];\n",
    "x = []#*topNRDoc\n",
    "\n",
    "for i in range(min(topNRDoc, len(tlist))):\n",
    "    Pmax = max(tlist)\n",
    "    #if least == 0:\n",
    "        #break\n",
    "    index = tlist.index(Pmax);\n",
    "    x.append(docList[index])\n",
    "    tlist[index] = -1;\n",
    "\n",
    "    \n",
    "rdl = len(x)\n",
    "if rdl == 0:\n",
    "    print(\"No relevent document found\")\n",
    "    \n",
    "else:\n",
    "    prt = \"\"\n",
    "    for i in range(rdl):\n",
    "        display(str(i+1) + ') ' + reviews[x[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
